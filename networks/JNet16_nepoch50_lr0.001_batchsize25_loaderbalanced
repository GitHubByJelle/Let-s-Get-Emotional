digraph {
	graph [size="19.05,19.05"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1611866022080 [label="
 (1, 7)" fillcolor=darkolivegreen1]
	1611916238272 [label=SoftmaxBackward]
	1611916238560 -> 1611916238272
	1611916238560 [label=AddmmBackward]
	1611916238512 -> 1611916238560
	1611914091648 [label="fc.2.bias
 (7)" fillcolor=lightblue]
	1611914091648 -> 1611916238512
	1611916238512 [label=AccumulateGrad]
	1611916238464 -> 1611916238560
	1611916238464 [label=ReluBackward0]
	1611916238608 -> 1611916238464
	1611916238608 [label=AddmmBackward]
	1611916238800 -> 1611916238608
	1611914089088 [label="fc.0.bias
 (1024)" fillcolor=lightblue]
	1611914089088 -> 1611916238800
	1611916238800 [label=AccumulateGrad]
	1611916238752 -> 1611916238608
	1611916238752 [label=ViewBackward]
	1611927208048 -> 1611916238752
	1611927208048 [label=MaxPool2DWithIndicesBackward]
	1611927208240 -> 1611927208048
	1611927208240 [label=ReluBackward0]
	1611927208336 -> 1611927208240
	1611927208336 [label=NativeBatchNormBackward]
	1611927208432 -> 1611927208336
	1611927208432 [label=ThnnConv2DBackward]
	1611927208624 -> 1611927208432
	1611927208624 [label=MaxPool2DWithIndicesBackward]
	1611927208816 -> 1611927208624
	1611927208816 [label=ReluBackward0]
	1611927208912 -> 1611927208816
	1611927208912 [label=NativeBatchNormBackward]
	1611927209008 -> 1611927208912
	1611927209008 [label=ThnnConv2DBackward]
	1611927209200 -> 1611927209008
	1611927209200 [label=MaxPool2DWithIndicesBackward]
	1611927209392 -> 1611927209200
	1611927209392 [label=ReluBackward0]
	1611927209488 -> 1611927209392
	1611927209488 [label=NativeBatchNormBackward]
	1611927209584 -> 1611927209488
	1611927209584 [label=ThnnConv2DBackward]
	1611927209776 -> 1611927209584
	1611927209776 [label=MaxPool2DWithIndicesBackward]
	1611927209968 -> 1611927209776
	1611927209968 [label=ReluBackward0]
	1611927210064 -> 1611927209968
	1611927210064 [label=NativeBatchNormBackward]
	1611927210160 -> 1611927210064
	1611927210160 [label=ThnnConv2DBackward]
	1611927210352 -> 1611927210160
	1611913753792 [label="layer1.0.weight
 (25, 1, 3, 3)" fillcolor=lightblue]
	1611913753792 -> 1611927210352
	1611927210352 [label=AccumulateGrad]
	1611927210304 -> 1611927210160
	1611913754560 [label="layer1.0.bias
 (25)" fillcolor=lightblue]
	1611913754560 -> 1611927210304
	1611927210304 [label=AccumulateGrad]
	1611927210112 -> 1611927210064
	1611914483008 [label="layer1.1.weight
 (25)" fillcolor=lightblue]
	1611914483008 -> 1611927210112
	1611927210112 [label=AccumulateGrad]
	1611927209872 -> 1611927210064
	1611913882688 [label="layer1.1.bias
 (25)" fillcolor=lightblue]
	1611913882688 -> 1611927209872
	1611927209872 [label=AccumulateGrad]
	1611927209728 -> 1611927209584
	1611913880256 [label="layer2.0.weight
 (64, 25, 2, 2)" fillcolor=lightblue]
	1611913880256 -> 1611927209728
	1611927209728 [label=AccumulateGrad]
	1611927209680 -> 1611927209584
	1611913880960 [label="layer2.0.bias
 (64)" fillcolor=lightblue]
	1611913880960 -> 1611927209680
	1611927209680 [label=AccumulateGrad]
	1611927209536 -> 1611927209488
	1611913880640 [label="layer2.1.weight
 (64)" fillcolor=lightblue]
	1611913880640 -> 1611927209536
	1611927209536 [label=AccumulateGrad]
	1611927209296 -> 1611927209488
	1611913879936 [label="layer2.1.bias
 (64)" fillcolor=lightblue]
	1611913879936 -> 1611927209296
	1611927209296 [label=AccumulateGrad]
	1611927209152 -> 1611927209008
	1611913743104 [label="layer3.0.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	1611913743104 -> 1611927209152
	1611927209152 [label=AccumulateGrad]
	1611927209104 -> 1611927209008
	1611913740928 [label="layer3.0.bias
 (128)" fillcolor=lightblue]
	1611913740928 -> 1611927209104
	1611927209104 [label=AccumulateGrad]
	1611927208960 -> 1611927208912
	1611913743360 [label="layer3.1.weight
 (128)" fillcolor=lightblue]
	1611913743360 -> 1611927208960
	1611927208960 [label=AccumulateGrad]
	1611927208720 -> 1611927208912
	1611913744064 [label="layer3.1.bias
 (128)" fillcolor=lightblue]
	1611913744064 -> 1611927208720
	1611927208720 [label=AccumulateGrad]
	1611927208576 -> 1611927208432
	1611913742976 [label="layer4.0.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	1611913742976 -> 1611927208576
	1611927208576 [label=AccumulateGrad]
	1611927208528 -> 1611927208432
	1611913742336 [label="layer4.0.bias
 (256)" fillcolor=lightblue]
	1611913742336 -> 1611927208528
	1611927208528 [label=AccumulateGrad]
	1611927208384 -> 1611927208336
	1611914091456 [label="layer4.1.weight
 (256)" fillcolor=lightblue]
	1611914091456 -> 1611927208384
	1611927208384 [label=AccumulateGrad]
	1611927208144 -> 1611927208336
	1611914091008 [label="layer4.1.bias
 (256)" fillcolor=lightblue]
	1611914091008 -> 1611927208144
	1611927208144 [label=AccumulateGrad]
	1611916238704 -> 1611916238608
	1611916238704 [label=TBackward]
	1611927208288 -> 1611916238704
	1611914091712 [label="fc.0.weight
 (1024, 1024)" fillcolor=lightblue]
	1611914091712 -> 1611927208288
	1611927208288 [label=AccumulateGrad]
	1611916238416 -> 1611916238560
	1611916238416 [label=TBackward]
	1611916238656 -> 1611916238416
	1611914088576 [label="fc.2.weight
 (7, 1024)" fillcolor=lightblue]
	1611914088576 -> 1611916238656
	1611916238656 [label=AccumulateGrad]
	1611916238272 -> 1611866022080
}
